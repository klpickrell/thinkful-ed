{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Guild - Natural Language Processing (Part 1)\n",
    "\n",
    "\n",
    "Natural language processing (NLP) is the algorithmic processing of natural language.  This tautology begs the question: what do we mean by natural language?  What do you think of when you hear the word \"natural\"?  Is a programming language natural?\n",
    "\n",
    "Unnatural hello world in C++:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{verbatim}\n",
    "#include <cstdio>\n",
    "int main( int, char** )\n",
    "{\n",
    "    std::fprintf( stdout, \"Unnatural Hello World!\\n\" );\n",
    "    return 0;\n",
    "}\n",
    "\\end{verbatim}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural hello world in Python:\n",
    "\n",
    "`print \"Natural Hello World!\"`\n",
    "\n",
    "While we can argue whether one programming language is more natural than another, it's tough to argue that they aren't inherently simpler than the equivalence of the following in English:\n",
    "\n",
    "*Print \"Hello World!\" to the screen.*\n",
    "\n",
    "or\n",
    "\n",
    "*Kindly print the exclamation: \"Hello World\" to the terminal, followed by an exclamation point.*\n",
    "\n",
    "By natural, we tend to think of the \"natural\" way one speaks.  When we think about natural language processing, we have a sense that it is somewhat more difficult to code against because there are a lot more possibilities to handle.  Programming languages are relatively simple with very explicit rules and a small number of keywords.  This difference between programming languages and natural language can be attributed to the differences in the grammars and vocabulary.  The grammar for english is much more \"free\" and less restrictive and the vocabulary is significantly larger.  Programming languages typically have around 50 keywords at most.  A working english vocabulary probably uses around 15,000 words.  Have you ever set out to process natural language in some way and first had this thought, \"I know.  I'll build a bunch of regular expressions to try to match things.\" only to find that it sort of works up to a point, but then breaks down in many cases, probably exhausting you in the process from having to come up with more and more clever regexes?  Part of this is simply that regexes can be tricky, but part of this is a fundamental error in algorithmic assumptions.  We'll explain this error in a bit, but for know, just understand that there is a well-studied relationship between the complexity of the grammar that you are trying to parse and the type of parser that is sophisticated enough for the grammar.  This study of formal languages is a branch of computability and complexity theory that was largely instigated by Noam Chomsky and Marcel-Paul Schutzenberger's work in the late 1950s.  Their work created a hierarchy of grammars along with the associated automata that are required to regonize the grammar:\n",
    "<img src=\"chomsky2.jpg\">"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{figure}[h!]                                                                                                     \n",
    "  \\centerline{\\includegraphics[width=0.5\\linewidth]{chomsky2.jpg}}                                                     \n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simpler the grammar, the simpler the algorithm that can recognize it.  So when we are trying to handle a phrase-structured language like English with regular expressions, which are suited for the much simpler class of regular grammars, we just don't have the firepower to recognize the constructs like we would like.\n",
    "\n",
    "NLP is a discipline that also draws heavily on computability and information theory.  To gain an intuitive understanding of this relationship between information and language, read the following quote in which the consonants have been removed and decide who said it:\n",
    "\n",
    "\"ouoe a ee ea ao...\"\n",
    "\n",
    "Pretty tough, right?  Try the same thing with just the consonants:\n",
    "\n",
    "\"Frscr nd svn yrs g...\"\n",
    "\n",
    "One reason this is slightly easier is because the consonants together carry more information about the word than the vowels do.  This is why whole languages can exist, Hebrew for one, which written form contains no vowels, vowel pointings aside.\n",
    "\n",
    "There are a lot of rabbit holes for us here.  Thankfully, it's not strictly necessary to have a complete understanding of automata theory and formal languages or information theory to get along in NLP.  It does, however, help considerably to have a general notion of the theoretical grounds you can stumble into as well as the kinds of software required to parse certain kinds of grammars, and generally where you are playing along the Chomsky hierarchy.\n",
    "\n",
    "### Oversimplification as an NLP strategy\n",
    "\n",
    "For a number of reasons, Occam's razor, avoiding hasty optimization, it makes a lot of sense to start with the simplest possible solution to extracting information from text by assuming that the language obeys certain rules, which it might not theoretically, but tends to do in practice.  This allows us to essentially code against a lower level on the hieararchy and still get pretty good results.  This tends to work because, for the most part, any kind of natural language you encounter is a subset of what can be produced by a grammar.  What can be produced, we typically call \"syntax\" while what makes sense we call \"semantic\".  Some syntactically correct sentences can be semantic nonsense.  The most famous example is probably Noam Chomsky's \"Colorless green ideas sleep furiously.\"  But it's easy to imagine syntactically correct sentences that are semantic nonsense like \"Invisible rocks smell the color five.\"\n",
    "\n",
    "Let's see how this strategy can play out with an example I recently encountered while attempting to extract wedding/reception venue price information from free-text descriptions.  I started with a list of strings, each one representing the price of a single venue:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{verbatim}\n",
    "'site rental start at $3,500.00 + catering starts at $55/pp'                                                           \n",
    "'site rental starts at $3,405.00'                                                                                      \n",
    "'rental fee starting at $650.00 + catering starts at $70/pp'                                                            \n",
    "'Rental fee starting at: $3,500, Ceremony fee starting at: $800'                                                       \n",
    "'Rental fee starting at: $750, Price per person starting at: $110, Ceremony fee starting at: $250'                     \n",
    "'Price per person starting at: $99.00, Ceremony fee starting at: $1,500 '                                              \n",
    "'$250 for just site rental; packages start at $3,395.00'                                                                \n",
    "'site rental starts at $375 + catering starts at $120/pp'                                                              \n",
    "'Rental and Catering: $113 to $124 per person | Ceremony Fee: $5 per person'                                            \n",
    "'site rental starts at $2,295.00'                                                                                      \n",
    "'site rental starts at $4,500.00'                                                                                      \n",
    "'Rental Fee: $5,500 to $7,500, based on weekday | Catering: $85 to $155 per person'                                    \n",
    "'Rental and Catering: $85 to $150 per person | Ceremony Fee: $750'                                                      \n",
    "'Rental and Catering: $99 to $130 per person'                                                                          \n",
    "'rental starts at $1,000.00'                                                                                            \n",
    "'Rental fee starting at: $4,000,  Price per person starting at: $35.00 '                                                \n",
    "'Rental Fee: $4,950 to $7,452 | Catering: $85+ per person'                                                              \n",
    "'Rental fee starting at: $500, Price per person starting at:$85, Ceremony fee starting at: $600'                        \n",
    "'Rental and Catering: $85 to $150 per person | Ceremony Fee: $800'                                                      \n",
    "'Rental fee starting at: $2,050'                                                                                        \n",
    "'Site Fee: $4,000, Price per person starting at: $150'                                                                  \n",
    "'Rental Fee: $6,500 to $10,500 | Catering: varies, based on caterer chosen'                                            \n",
    "'Rental Fee: $2,500 | Catering: $110 to $140 per person | Ceremony Fee: $750 to $1,000'                                \n",
    "'Ceremony: $1,500+, Site Fee: $3,000+, Per Person: $50+'                                                                \n",
    "'Price per person starting at $110'                                                                                    \n",
    "'Rental Fee: $2,500 to $6,800, based on weekday | Catering: $15 to $50 per person'                                      \n",
    "'Elopement Package: $1,215 or $1,690 based on guest count'                                                              \n",
    "'Rental: 8,000+'                                                                                                        \n",
    "'Rental: 500+  Catering: 7,000+ '                                                                                      \n",
    "'Price per person starting at: $68, Ceremony fee starting at: $500'                                                    \n",
    "'Rental Fee: $4,000 to $5,000 | Catering: $55 to $82 per person'                                                        \n",
    "'Rental and Catering: $58 to $84 per person'                                                                            \n",
    "'Rental and Catering: $133 to $175 per person'\n",
    "\\end{verbatim}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks daunting at first.  Descriptions are all over the place, sometimes they have rental fees, sometimes per-person fees, sometimes a range is indicated, not to mention how this stuff is ordered within the string.  But let's take a few examples and try to simplify them by cherry-picking keywords and ignoring everything else in order to get our minds around how to tackle this problem.  Take the first three, for example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{verbatim}\n",
    "'site rental start at $3,500.00 + catering starts at $55/pp'\n",
    "'site rental starts at $3,405.00'\n",
    "'rental fee starting at $650.00 + catering starts at $70/pp'\n",
    "\\end{verbatim}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine you are like a three year old version of my son who loves trains and snakes and juice boxes.  As you encounter the events of your day, you can imagine that the wonders you encounter are simplified into a version that looks a lot like this:\n",
    "\n",
    "_stuff_ _stuff_ _stuff_ _stuff_ **snake** _stuff_ _stuff_ _stuff_ **train** _stuff_ _stuff_ **juice box** _stuff_\n",
    "\n",
    "Let's build a little recognizer that does that same kind of thing with our price strings.  First let's remove extraneous tokens and try to boil it down the simplest possible sentence that contains our information, preserving the order:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{verbatim}\n",
    "'rental 3500 catering 55'\n",
    "'rental 3405'\n",
    "'rental 650 catering 70'\n",
    "\\end{verbatim}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked pretty nicely, and in fact building a parser with this oversimplified view of reality seems tractable now.  Let's try a few more at random:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{verbatim}\n",
    "'Price per person starting at: $68, Ceremony fee starting at: $500'\n",
    "'Rental and Catering: $99 to $130 per person'\n",
    "'Rental Fee: $2,500 | Catering: $110 to $140 per person | Ceremony Fee: $750 to $1,000'\n",
    "\\end{verbatim}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simplified becomes:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{verbatim}\n",
    "'person 68 ceremony 500'\n",
    "'rental catering 99 130 person'\n",
    "'rental 2500 catering 110 140 ceremony 750 1000'\n",
    "\\end{verbatim}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit confusing here, but still much simpler than the original.  On the second string, it's hard to know what the range applies to.  But it's enough for our purposes to see that it is a per-person cost equivalent to a catering cost.  One might envision a recursive context-free grammar similar to the following to be able to handle these boiled down strings:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{verbatim}\n",
    "<R> -> rental, site, ground, inclus\n",
    "<C> -> cerem\n",
    "<P> -> catering, person, pp\n",
    "<p1> -> '\\d+'\n",
    "<price> -> <p1> 'to|-' <p1>\n",
    "<price> -> <p1>\n",
    "<rental_price> -> <R> <price> | <price> <R>\n",
    "<ceremony_price> -> <C> <price> | <price> <C>\n",
    "<catering_price> -> <P> <price> | <price> <P>\n",
    "<aspect_price> -> <rental_price> | <ceremony_price> | <catering_price>\n",
    "<description> -> <aspect_price> <description> | epsilon\n",
    "\\end{verbatim}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grammar would be consumed by a tool that would generate a parser for you.  The parser would then produce a syntax tree from the grammar given an input string.  The pricing information could then be culled from the syntax tree in no worse than `O=lg(n)` time.  Our general process was:\n",
    "\n",
    "* Clean the input\n",
    "* Scan the input for keywords and price tokens\n",
    "* Operate on the tokens and produce a parse tree based on our grammar\n",
    "\n",
    "The second process is called `scanning` or `lexing` where the output is a sequence of `lexemes` or tokens to be processed, and the third process is called `parsing`.  Tools like lex/yacc or flex/bison can consume lists of regular expressions and EBNF grammars and produce parsers for you.\n",
    "\n",
    "One very interesting connection here...  notice the production:\n",
    "\n",
    "`<rental_price> -> <R> <price> | <price> <R>`\n",
    "\n",
    "If you imagine reading a string and processing tokens from left to right, and if it could be assumed that the order of the price and rental keyword would always be the same, we wouldn't need both rules.  In fact, if strings were simply of the form `<R> <price> <C> <price> <P> <price>` where any of the clauses were optional we could read along, find either 'rental' or 'cermony' or 'catering' and then the next thing we extract would be the price for that aspect of the venue.  But if we got a string and the first thing we read was a price, we wouldn't know how it was applied until we kept reading **_holding the price in memory_** and discovered one of our keywords somewhere later in the string.  The difference here is, in the first example, **_state alone is enough_**, but in the second example **_you need to store something_**.  If you look back up at the Chomsky hierarchy, you'll notice that the most crude machine for regular grammars is a finite state machine, while the next level up is a push-down automaton.  It is push-down because it is employing a stack which is a simple way to implement hold-for-later in memory!  So in our example if you imagine each of the tokens in the string being popped off left to right, when we encounter a price without a keyword, we would need to push the price back onto the stack until we could pop both the price and the keyword so we know how the price is applied.\n",
    "\n",
    "Here is some example output from a pushdown automaton that produces the averages of any indicated ranges for the three-tuple *(catering fee, ceremony fee, site rental fee)*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{verbatim}\n",
    " ((45, None, 2000),                                                                                                               \n",
    "  u'site rental starts at $2,000 + catering starts at $45/pp'),                                                                   \n",
    " ((None, None, 400), u'Rental Fee: Starting At $400'),                                                                            \n",
    " ((None, None, 2650), u'Rental fee starting at: $2,650'),                                                                         \n",
    " ((40, 1000, None),                                                                                                               \n",
    "  u'Catering: Starting around $40 per person; Ceremony Fee: $1,000 '),                                                            \n",
    " ((80, None, None), u'Price per person starting at $80'),                                                                         \n",
    " ((None, None, None), u'STILL WAITING FOR PRICING'),                                                                              \n",
    " ((120, 2500, 1000),                                                                                                              \n",
    "  u'Rental Fee:  Starting at $1,000 Ceremony Fee: Starting at $2,500 \n",
    "    Catering Fee: Starting at  $120 per person'),                \n",
    " ((None, None, 1500), u'Rental fee starting at: $1,500'),                                                                         \n",
    " ((72, None, 2100),                                                                                                               \n",
    "  u'Rental fee starting at: $2,100, Price per person starting at: $72.00'),                                                       \n",
    " ((None, None, 2000), u'$2000 rental fee'),\n",
    " ((9000, None, 750),                                                                                                              \n",
    "  u'Estimated Price- $500- $1000 rental fee/$6,000-$12,000 catering minimum'),\n",
    "\\end{verbatim}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good for an oversimplified view.  In fact, it's greater than 99% good.  So when you are trying to extract information from text, which is the heart of language processing, give a lot of thought to exactly the information you are trying to extract and exactly what is needed to extract it, and no more.  Don't get too bogged down by the details.  As long as you see the snakes and trains and have the occasional juice box along the journey, you're doing pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harder, Better, Faster, Stronger\n",
    "\n",
    "The above is a simple example of an *Information Extraction* (IE) system.  As we deal with more sophisticated examples of natural language, it will be helpful to think of text processing as a pipeline with roughly the following stages:\n",
    "\n",
    "* Sentence cleaning and segmentation\n",
    "* Sentence tokeniziation\n",
    "* Part of speech tagging\n",
    "* Entity detection\n",
    "* Relationship detection\n",
    "\n",
    "We've already been introduced to the first two in the last example.  Let's take a look at part of speech (POS) tagging.  The goal in POS tagging is to identify the parts of speech of a sentence.  Parts of speech are things like nouns, adjectives, personal pronouns, etc.  It is not always possible to identify the part of speech based on word lookup.  For example, in \"The wind blew.\" and \"Please wind the clock.\" lookup of wind would fail in one case unless the semantics of the sentence were analyzed.  Clearly, something more sophisticated than a dictionary lookup is needed in order to accurately identify which parts of speech a word is playing at the moment.  Part of speech taggers generally fall into one of two categories:\n",
    "\n",
    "* Rule-based taggers\n",
    "* Stochastic taggers\n",
    "\n",
    "Rule based taggers attempt to build a set of rules that cover most cases.  Rules would be things like \"*wind* means air movement if one of the adjacent word's root is *blow*\".  Most rule-based taggers have hundreds of rules and use supervised learning techniques to minimize the error of the applied tags.\n",
    "\n",
    "Stochastic taggers operate by building a model, usually a hidden markov model (HMM), that gives the most likely part of speech for a given word based on the path you traversed when encountering a word.  HMM's can be thought of as a state-transition diagram with probabilities along the transition edges.  So in our above example, it's likely the model would encounter the word *blew* and update it's part of speech prediction for the word *wind*.\n",
    "\n",
    "You can see in both cases that some amount of training data is required in order to learn the appropriate rules or probabilities.  Both types of POS taggers tend to use supervised learning techniques or some form of expectation maximization, which is the typical algorithm in which most machine learning problems are formulated.\n",
    "\n",
    "Alright, let's do some stuff with part of speech tagging..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags are:\n",
      "(u'A', u'DT')\n",
      "(u'big', u'JJ')\n",
      "(u'yellow', u'JJ')\n",
      "(u'dog', u'NN')\n",
      "(u'jumped', u'VBD')\n",
      "(u'over', u'IN')\n",
      "(u'the', u'DT')\n",
      "(u'moon', u'NN')\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "sample = 'A big yellow dog jumped over the moon.'\n",
    "tb = TextBlob(sample)\n",
    "print \"Tags are:\\n{}\".format('\\n'.join(str(i) for i in tb.tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output is each token followed by the detected part of speech code in Penn Treebank format, courtesy of the University of Pennsyvania.  You can reference the complete list at \n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "Let's extract the nouns and verbs, which by lookup are:\n",
    "\n",
    "<pre>12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural</pre>\n",
    "\n",
    "Since they all begin with 'N', let's just check the first letter for compactness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just the nouns:  [u'dog', u'moon']\n"
     ]
    }
   ],
   "source": [
    "print( 'Just the nouns:  {}'.format( [ i[0] for i in tb.tags if i[1][0] == 'N' ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, verbs are codes that begin with 'V':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just the verbs:  [u'jumped']\n"
     ]
    }
   ],
   "source": [
    "print( 'Just the verbs:  {}'.format( [ i[0] for i in tb.tags if i[1][0] == 'V' ] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob is a package that provides a thin layer over Python's Natural Language Toolkit (NLTK).  It provides a lot of convenience functions as well as a number of taggers.  Here's a little test drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: defaultdict(<type 'int'>, {u'of': 1, u'strings': 3, u'together': 1, u'string': 1})\n",
      "\n",
      "2: [Sentence(\"First, I bought a car.\"), Sentence(\"Then, I bought an air freshener.\")]\n",
      "\n",
      "3: 5125551212\n",
      "\n",
      "4: ['Joe', 'and', 'Mary'], ['and', 'Mary', 'had'], ['Mary', 'had', 'coffee']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = 'Strings of strings string together strings.'\n",
    "print( '1: {}\\n'.format(TextBlob(s).word_counts) )\n",
    "\n",
    "s = 'First, I bought a car.  Then, I bought an air freshener.'\n",
    "print( '2: {}\\n'.format(TextBlob(s).sentences) )\n",
    "\n",
    "s = '(512)555-1212'\n",
    "print( '3: {}\\n'.format(TextBlob(s).stripped) )\n",
    "\n",
    "s = 'Joe and Mary had coffee.'\n",
    "print( '4: {}\\n'.format(', '.join(str(i) for i in TextBlob(s).ngrams())) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob even contains a built-in sentiment analyzer trained on a somewhat generic corpus.  If you were at the last meeting, you saw where I built a sentiment analyzer for movie ratings from scratch using a naive bayes classifier on a bag-of-words model.  While that is usually preferable, as you want results tailored to the corpus you are interacting with, it's nice to have simple built-ins to give you a chance to try some things out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.625, subjectivity=0.6)\n",
      "Sentiment(polarity=0.5, subjectivity=0.55)\n",
      "Sentiment(polarity=0.4166666666666667, subjectivity=0.5)\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment(polarity=-0.1, subjectivity=0.2)\n",
      "Sentiment(polarity=-1.0, subjectivity=0.9)\n"
     ]
    }
   ],
   "source": [
    "print( TextBlob( \"I absolutely love this ski resort!\" ).sentiment )\n",
    "print( TextBlob( \"I love most ski resorts.\" ).sentiment )\n",
    "print( TextBlob( \"Resort life is fine, I guess.\" ).sentiment )\n",
    "print( TextBlob( \"Skiing relies on gravity.\" ).sentiment )\n",
    "print( TextBlob( \"I'm really not that into skiing.\" ).sentiment )\n",
    "print( TextBlob( \"I hate skiing!\" ).sentiment )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polarity ranges from -1.0 being a very negative sentiment to 1.0 being a very positive sentiment.  Subjectivity is a measure of how objective/subjective the statement is from 0 (most objective) to 1.0 (most subjective).\n",
    "\n",
    "### Noun Phrases\n",
    "\n",
    "Once we have found the various parts of speech of our text, we can start aggregating them into entities that we can use in our IE machine.  Noun phrases are nouns paired with any modifiers like \"yellow dog\", \"big purse\", \"urban outfit\".  They tend to be things we are interested in products, movies, wedding venues, or anything else that is described with natural language.  Once we have a POS-tagged sentence, we can aggregate tags using a *chunker* into informational chunks that let us do more interesting things...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Ok Siri', u'Modern Family', u'Edward Norton']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'Ok Siri, whats that episode of Modern Family with Edward Norton?'\n",
    "TextBlob.np_extractor.extract( sample )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...**_Detected OK Siri, search titles with tags [Modern Family, Edward Norton]_**...\n",
    "<img src='mfedward.jpg'>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\begin{figure}[h!]                                                                                                     \n",
    "  \\centerline{\\includegraphics[width=0.5\\linewidth]{mfedward.jpg}}                                                     \n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...*hold for applause*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'historic plantation house', u'waterfront backdrop']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'I really want to have a wedding at an historic plantation house \\\n",
    "          with a waterfront backdrop.'\n",
    "TextBlob.np_extractor.extract( sample )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coupling this info with our sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.1, subjectivity=0.1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(sample).sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to build a picture of who people are based on the language they use, what things they are interested, and how they feel about those things.  Since noun phrases contain interesting nuggets of information in a natural language description, we can start to measure differences and similarities between natural language descriptions.  It stands to reason that the more interesting things two product descriptions have in common, especially if they are rarely ocurring things, the more similar the descriptions will be.  Here are some examples of high similarity discovered from our description data in our Concierge program using an algorithm I built that clusters by description similarity relying heavily on noun-phrase co-ocurrence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ \"Where romance, elegance and history meet to create an exquisite facility for the wedding celebration of your dreams. Red Bank's landmark waterfront Molly Pitcher Inn prides itself in its impeccable hospitality services and is the recipient of the Five Star Diamond Award.\",\"I love the Molly Pitcher Inn for your special day! This historical landmark located in Red Bank, NJ is absolutely lovely! Host your cocktail reception on their tented promenade overlooking the Navesink River and dance your wedding night away in the water view Ballroom for 200 guests. A great perk of this venue is the flexibility and customization to include, ceremony, reception, rehearsal dinner, bridal lunches and farewell breakfast, if you wish.\",\n",
    "\n",
    "\"The boutique Oyster Point Hotel offers contemporary elegance, outstanding continental fusion cuisine and a staff dedicated to exceeding expectations to create the wedding of your dreams. The unparalleled sweeping views of the picturesque Navesink River will make your wedding a night to remember.\",\"This venue is owned by the same as Molly Pitcher, it is a little more modern and sleek, very cool ballroom as well! It is also on the water in Red Bank and they have overnight accommodations for guests which is awesome. Their Riviera package has options for around $129 per person,  plus so many amazing options to work with your vision! I know Bill here, I can get you in contact with him anytime!\" ]\n",
    "\n",
    "--\n",
    "\n",
    "\n",
    "[ \"This plantation house is the epitome of southern elegance. The grounds are huge and the perfect place to set up a big white tent! Plus there are beautiful live oak trees to hang lanterns or twinkly lights for a romantic, rustic atmosphere.\", \n",
    "\n",
    "\"This charming waterfront venue is the perfect location for a breezy, southern reception. Located on gorgeous St. Helena Island, this plantation house is shaded by oak trees and sits on acres of marsh-front property. It also serves as an inn so you and your bridal party can stay for the weekend and have easy access to all the festivities.\" ]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next meeting, we'll talk more about entity recognition, discuss various techniques for building corpora and talk some about building the predictive models that underlie many of these algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
